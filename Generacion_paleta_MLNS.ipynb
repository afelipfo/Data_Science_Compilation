{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBMLRJHcmaJ8"
   },
   "source": [
    "# MAIA - Machine Learning No Supervisado\n",
    "## Microproyecto 1: Generación de paleta de colores utilizando MLNS\n",
    "\n",
    "### Estudiantes:\n",
    "- Claudia Agudelo\n",
    "- Felipe Flórez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Omu6-4_ljrO_"
   },
   "source": [
    "### **Descripción de la aplicación:**\n",
    "\n",
    "Una paleta de colores es un conjunto de tonos utilizados para crear armonía visual y transmitir emociones en diversas formas de arte. Ante la creciente demanda de herramientas que asistan a creadores en la selección de colores, una aplicación que genere paletas a partir de imágenes sería muy útil. El reto es desarrollar un método automatizado que no solo identifique colores dominantes, sino que también cree combinaciones visualmente atractivas. Esto optimizaría el proceso creativo, garantizaría consistencia visual, facilitaría el diseño para principiantes y mejoraría la comunicación visual. Una posible solución es aplicar técnicas de machine learning no supervisado para generar paletas basadas en la distribución de colores en imágenes, con aplicaciones en diversos campos como marketing, arte y estudios ambientales.\n",
    "\n",
    "#### **Objetivo**\n",
    "\n",
    "Desarrollar un método, basado en técnicas de agrupación, que permita extraer los tonos de una imagen y generar un muestrario de los colores presentes en esta.\n",
    "\n",
    "#### **Conjunto de datos**\n",
    "\n",
    "Los datos están asociados con imágenes de obras de arte. Pueden ser descargados a partir de este [WikiArt](https://www.kaggle.com/datasets/steubk/wikiart/data)\n",
    "\n",
    "#### **Actividades a realizar**\n",
    "\n",
    "1. Recopilación de las imágenes a partir del repositorio. La idea es seleccionar un conjunto diverso de muestras en diferentes estilos artísticos.\n",
    "\n",
    "2. Preparación de las imágenes para el entrenamiento y prueba del modelo. Para este paso construir un pipeline que integre las transformaciones que se consideren adecuadas.\n",
    "\n",
    "3. Desarrollo del modelo de agrupación para identificar los colores presentes en las imágenes.\n",
    "\n",
    "4. Creación de un modelo que transforme los grupos de colores identificados en un muestrario representativo. Adicionalmente, se debería mostrar la distribución de los colores de la imagen en un espacio de dos dimensiones.\n",
    "\n",
    "\n",
    "* El algoritmo de agrupación a utilizar queda a consideración de cada grupo, pero es importante justificar la elección.\n",
    "\n",
    "* El número de colores de una paleta generada a partir de una imagen debería estar entre 5 y 7.\n",
    "\n",
    "* Para la visualización de la distribución de colores seleccionar un método como t-SNE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hiLhrsMzi0K0"
   },
   "source": [
    "## Carga de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizador de ejecución con GPU\n",
    "!pip install numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uso de temporizador para ejecución\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i5uhfTF0i0K7"
   },
   "outputs": [],
   "source": [
    "# Librerías a utilizar\n",
    "from numba import cuda\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Visualización de datos\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Procesamiento de datos\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Modelos de Machine Learning No Supervisado\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "\n",
    "# Reducción de dimensionalidad\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Métricas de evaluación\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score, adjusted_rand_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Ignorar warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasos:\n",
    "\n",
    "1. Se define una ruta a una carpeta (main_folder) que contiene subcarpetas con imágenes.\n",
    "2. Se selecciona aleatoriamente una subcarpeta y luego se copian un máximo de 100 imágenes de esa carpeta a una nueva carpeta llamada imagenes.\n",
    "   \n",
    "El objetivo es almacenar las imágenes en un directorio específico para su posterior procesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EPm3z8B_i0K_",
    "outputId": "f6fd39c0-8c17-430d-84d9-960865fb7239"
   },
   "outputs": [],
   "source": [
    "#Ruta de la carpeta de las imagenes\n",
    "main_folder = \"archive\"\n",
    "\n",
    "#Obtener las carpetas de estilos, filtrando solo las que son directorios\n",
    "folders = [folder for folder in os.listdir(main_folder) if os.path.isdir(os.path.join(main_folder, folder))]\n",
    "\n",
    "#Seleccionar 10 carpetas aleatorias\n",
    "selected_folders = random.sample(folders, 1)\n",
    "\n",
    "#Crear una carpeta para almacenar las imágenes seleccionadas en el escritorio\n",
    "output_folder = \"imagenes\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "#Seleccionar imágenes aleatorias de cada carpeta seleccionada\n",
    "for folder in selected_folders:\n",
    "    folder_path = os.path.join(main_folder, folder)\n",
    "    images = os.listdir(folder_path)\n",
    "\n",
    "    #Verificar si hay menos de 100 imágenes\n",
    "    num_images_to_select = min(100, len(images))\n",
    "    selected_images = random.sample(images, num_images_to_select)\n",
    "\n",
    "    for image in selected_images:\n",
    "        src_path = os.path.join(folder_path, image)\n",
    "        dst_path = os.path.join(output_folder, image)\n",
    "        shutil.copy(src_path, dst_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ep2Obty1v2v-"
   },
   "source": [
    "## Carga de imagenes almacenadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHOA9w5g1_io"
   },
   "source": [
    "Creamos una función para cargar imágenes desde un directorio al espacio de trabajo. Luego, visualizamos una imagen de muestra y analizamos sus características, incluyendo: la visualización de la imagen, los valores mínimo y máximo de los píxeles, sus dimensiones (número de píxeles por dimensión), el número de canales de color, y el tipo de codificación de los píxeles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasos:\n",
    "1. Definimos la clase Image_loader, la cual permite cargar imágenes de una carpeta específica y visualizar las imágenes seleccionadas.\n",
    "2. Ahora hacemos uso del método loader, ya que este carga las imágenes basadas en índices proporcionados, y plotter permite mostrar la imagen y analizar sus propiedades (dimensiones, normalización, número de canales, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g8nGWkab9BsG"
   },
   "outputs": [],
   "source": [
    "# Constructor para cargar imagenes\n",
    "class Image_loader:\n",
    "    def __init__(self, folder_path, image_indices = (0,1)):\n",
    "        self.folder_path = folder_path\n",
    "        self.images = self.loader(image_indices)\n",
    "\n",
    "    # Lector de imágenes\n",
    "    def loader(self, image_indices = (0,1)):\n",
    "        images = []\n",
    "        image_folder = self.folder_path\n",
    "        image_paths = [os.path.join(image_folder, img) for img in os.listdir(image_folder) if img.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "        for img_path in image_paths[image_indices[0]: image_indices[1]]:\n",
    "            try:\n",
    "                image = Image.open(img_path)\n",
    "                images.append(np.array(image))\n",
    "            except Exception as e:\n",
    "                print(f\"Error al cargar la imagen {img_path}: {e}\")\n",
    "                images.append(None)\n",
    "        return images\n",
    "\n",
    "    # Graficador de imágenes\n",
    "    def plotter(self, index_image=0):\n",
    "        if index_image < 0 or index_image >= len(self.images):\n",
    "            print(f\"Índice fuera de rango. Hay {len(self.images)} imágenes disponibles.\")\n",
    "        else:\n",
    "            try:\n",
    "                selected_image = self.images[index_image]\n",
    "                if selected_image is not None:\n",
    "                    # Mostrar la imagen\n",
    "                    plt.imshow(selected_image)\n",
    "                    plt.axis('off')\n",
    "                    plt.show()\n",
    "\n",
    "                    # Dimensiones de la imagen\n",
    "                    height, width = selected_image.shape[:2]\n",
    "                    print(f\"Dimensiones: {height}x{width}\")\n",
    "\n",
    "                else:\n",
    "                    print(f\"La imagen en el índice {index_image} no es válida.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error al graficar la imagen: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasos:\n",
    "\n",
    "1. Ahora se utiliza la clase Image_loader previamente definida para cargar imágenes de la carpeta imagenes.\n",
    "2. Luego, seleccionamos un índice aleatorio de entre las imágenes cargadas y se visualiza la imagen correspondiente utilizando el método plotter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gZ1gnrk4C31U"
   },
   "source": [
    "Para ilustrar la manera en la que opera el anterior constructor, cargamos una serie de imágenes y procedemos a visualizar una imagen de ejemplo de nuestro interés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BtKkYtbh9zp1",
    "outputId": "0a1cced1-5b68-479d-cfca-cd8fca1303da"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Ruta de la carpeta de las imágenes\n",
    "image_folder = 'imagenes'\n",
    "\n",
    "# Imagen de ejemplo\n",
    "indices_imagenes = (0, 500)\n",
    "loader = Image_loader(folder_path=image_folder, image_indices=indices_imagenes)\n",
    "\n",
    "# Generar un valor aleatorio distinto de 0\n",
    "random_index = random.choice([i for i in range(indices_imagenes[0], indices_imagenes[1] + 1) if i != 0])\n",
    "\n",
    "# Usar el valor aleatorio en loader.plotter\n",
    "loader.plotter(index_image=random_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vzPXqNkEi0LB"
   },
   "source": [
    "## Preparación de datos y creación del Pipeline\n",
    "\n",
    "En la preparación de datos, se crea un pipeline con los siguientes pasos:\n",
    "- **Resizer**: Redimensiona todas las imágenes a un tamaño fijo de (128,128) píxeles.\n",
    "- **Normalizer**: Escala los valores de los píxeles al rango [0,1].\n",
    "- **Convertidor de color**: Convierte las imágenes en blanco y negro al formato RGB de tres canales.\n",
    "- **Squasher**: Aplana las imágenes en un array $(l,3)$, donde $l = m \\times n$, siendo $m$ y $n$ el alto y ancho de la imagen, respectivamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasos:\n",
    "\n",
    "1. Ahora haremos uso de la clase Resizer para redimensionar las imágenes a un tamaño especificado, que por defecto es 128x128 píxeles.\n",
    "2. Luego utilizaremos el método transform para redimensionar cada imagen y devolverla como un arreglo NumPy, lo que será útil para estandarizar todas las imágenes antes de aplicar algún análisis o procesamiento adicional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HBwuJyv7i0LD"
   },
   "outputs": [],
   "source": [
    "# Transformador para redimensionar imágenes\n",
    "class Resizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, size=(128, 128)):\n",
    "        self.size = size\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Usamos map para aplicar el resize a cada imagen en X\n",
    "        return np.array([self._resize_image(img_path) for img_path in X])\n",
    "\n",
    "    def _resize_image(self, img_path):\n",
    "        try:\n",
    "            image = Image.open(img_path).resize(self.size)\n",
    "            return np.array(image)\n",
    "        except Exception as e:\n",
    "            print(f\"Error de redimensionamiento de la imagen {img_path}: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasos:\n",
    "\n",
    "1. Ahora usamos la clase Normalizer para normalizar los valores de los píxeles de las imágenes dividiéndolos por 255.0, lo que los lleva a un rango de [0, 1].\n",
    "\n",
    "La normalización de los datos es importante cuando se utilizan algoritmos de aprendizaje automático, ya que ayuda a estabilizar el comportamiento de los modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformador para normalizar imágenes\n",
    "class Normalizer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Aseguramos que X sea un array de NumPy para realizar la normalización de forma eficiente\n",
    "        X = np.asarray(X, dtype=np.float32)\n",
    "        return X / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasos: \n",
    "\n",
    "1. Ahora usamos la clase InverseNormalizer para realizar el proceso inverso de la normalización, devolviendo los valores de los píxeles al rango original [0, 255]. Esto puede ser útil si es necesario visualizar las imágenes o restaurarlas después de realizar algún análisis o procesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformador inverso para denormalizar imágenes\n",
    "class InverseNormalizer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Aseguramos que X sea un array de NumPy para realizar la denormalización de forma eficiente\n",
    "        X = np.asarray(X, dtype=np.float32)\n",
    "        return (X * 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasos:\n",
    "\n",
    "1. Ahora utilizamos el transformador ColorConverter, el cual asegura que todas las imágenes tengan tres canales de color (Rojo, Verde, Azul). Si alguna imagen es en blanco y negro (2D), se duplica la imagen en los tres canales. Esto es importante para asegurarse de que todas las imágenes estén en un formato RGB adecuado antes de realizar el análisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformador para los canales R, B, G\n",
    "class ColorConverter(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Aseguramos que X sea un array de NumPy para realizar las operaciones de forma eficiente\n",
    "        X = np.asarray(X)\n",
    "\n",
    "        # Usamos list comprehension para simplificar la conversión\n",
    "        return np.array([self._convert_image(image) for image in X])\n",
    "\n",
    "    def _convert_image(self, image):\n",
    "        if image is not None:\n",
    "            # Si la imagen es en blanco y negro (2D), duplicamos los canales para hacerla RGB\n",
    "            if len(image.shape) == 2:\n",
    "                return np.stack((image,)*3, axis=-1)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasos:\n",
    "\n",
    "1. Ahora implementamos el transformador Squasher, el cual toma una imagen y convierte su forma original (alto, ancho, 3) en una forma aplanada de (número total de píxeles, 3). En otras palabras, se requiere aplanar la imagen para que cada fila represente un píxel y cada columna un canal de color (R, G, B). Esta transformación es clave cuando se desea aplicar algoritmos de agrupación (como K-means) que trabajarán con las características de los píxeles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conversor a 3 dimensiones\n",
    "\n",
    "class Squasher(BaseEstimator, TransformerMixin):\n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self,X):\n",
    "        squashed_images = []\n",
    "        for image in X:\n",
    "            if image is not None:\n",
    "                pixels = image.reshape(-1, 3)\n",
    "                squashed_images.append(pixels)\n",
    "            else:\n",
    "                squashed_images.append(None)\n",
    "        return np.array(squashed_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasos:\n",
    "\n",
    "1. Ahora definimos un pipeline de preprocesamiento que aplica tres pasos a cada imagen:\n",
    "1.1. Redimensionamiento a 128x128 píxeles.\n",
    "1.2. Conversión de los canales de color para asegurar que todas las imágenes estén en formato RGB.\n",
    "1.3. Normalización de los valores de los píxeles (de [0, 255] a [0, 1]).\n",
    "   \n",
    "Este pipeline se aplica a las primeras 500 imágenes almacenadas en la carpeta imagenes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2CMswFlYD61b"
   },
   "source": [
    "Veremos a continuación un ejemplo de la manera en la que los procedimientos Resizer,  Normalizer y ColorConverter funcionan. Generamos un pipeline de muestra y lo aplicamos a una imagen de ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5imsBCVZdKCB",
    "outputId": "7f65f19e-ccf9-4eac-ad73-5cc2028b7a12"
   },
   "outputs": [],
   "source": [
    "# Aplicación del pipeline de preprocesamiento para una imagen\n",
    "\n",
    "image_folder = \"imagenes\"\n",
    "\n",
    "#Crear la lista de rutas de las imágenes\n",
    "image_paths = [os.path.join(image_folder, img) for img in os.listdir(image_folder)]\n",
    "\n",
    "#Definir el pipeline\n",
    "preprocessing_pipeline = Pipeline([\n",
    "    ('resize', Resizer(size=(128, 128))),\n",
    "    ('color_convert', ColorConverter()),\n",
    "    ('normalize', Normalizer()),\n",
    "])\n",
    "\n",
    "#Aplicar el pipeline\n",
    "processed_images_array = preprocessing_pipeline.fit_transform(image_paths[0:500])\n",
    "\n",
    "print(f\"Se han procesado {len(processed_images_array)} imágenes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasos:\n",
    "\n",
    "1. Ahora generamos un índice aleatorio dentro del rango de las imágenes preprocesadas, excluyendo el índice 0. Utilizamos random.choice para elegir un número al azar dentro de los índices válidos de las imágenes contenidas en processed_images_array, implementamos el processed_images_array ya que contiene las imágenes que han pasado por el proceso de redimensionamiento, normalización y posible conversión a formato RGB. Finalmente, utilizamos plt.imshow de la biblioteca Matplotlib se utiliza para visualizar la imagen seleccionada. Esta función toma una matriz (que representa la imagen) y la despliega en un gráfico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 432
    },
    "id": "QMu_CQL6j2p4",
    "outputId": "929dee0a-4331-444e-a401-2e46e86fb5f0"
   },
   "outputs": [],
   "source": [
    "random_index = random.choice([i for i in range(1, len(processed_images_array))])\n",
    "\n",
    "# Seleccionar la imagen con ese índice aleatorio\n",
    "imagen_mat = processed_images_array[random_index]\n",
    "\n",
    "# Graficar la imagen\n",
    "plt.imshow(imagen_mat)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kvJ9PtUii0LE"
   },
   "source": [
    "## Selección de los modelos de agrupación, sus hiperparámetros y sus métricas\n",
    "En la presente sección, vamos a usar modelos diversos de agrupamiento para realizar la tarea de extracción de la paleta de colores característica, comparando la manera en como se comportan los modelos con los datos, validándolos con medidas de evaluación adecuadas para cada uno de ellos. Los modelos a utilizar son K-means, DBSCAN y Clustering Jerárquico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora iniciamos definiendo la clase de cluster de colores, estructurada de la siguiente manera:\n",
    "\n",
    "Función __init__:\n",
    "- method: Algoritmo de agrupación que se usará (por defecto, KMeans)\n",
    "- scoring_metric: Métrica de evaluación que se usará para validar la calidad de los grupos. Para este caso tenemos silhouette, calinski_harabasz, y davies_bouldin\n",
    "- param_grid: Conjunto de parámetros que se utilizarán para realizar una búsqueda de hiperparámetros con GridSearchCV\n",
    "- squashed: Define si las imágenes ya han sido aplanadas o no. Si es False, el código aplanará las imágenes de forma predeterminada\n",
    "- kwargs: Parámetros adicionales que se pasarán a los modelos de agrupación.\n",
    "\n",
    "Función get_scoring_metric:\n",
    "- silhouette_score: Mide qué tan similares son los objetos dentro de un grupo en comparación con otros grupos\n",
    "- calinski_harabasz_score: Evalúa la dispersión entre grupos en relación con la dispersión dentro de los grupos\n",
    "- davies_bouldin_score: Mide la relación entre la distancia entre los grupos y el tamaño de los grupos\n",
    "\n",
    "Función fit:\n",
    "- Método para entrenar un modelo de agrupación para cada imagen en el conjunto X (preprocesado).\n",
    "- Consideramos que si squashed es True, la imagen ya está aplanada (2D), de lo contrario, se aplana aquí (convertida de una matriz 3D de colores a una matriz 2D donde cada fila representa un píxel RGB). Según el valor de method, se elige un algoritmo de agrupación: KMeans, DBSCAN o AgglomerativeClustering.\n",
    "- GridSearchCV: Este proceso ajusta el modelo de agrupación mediante búsqueda de hiperparámetros, utilizando el conjunto de parámetros en param_grid y evaluando con la métrica seleccionada. Se selecciona el mejor modelo encontrado. Se guardan los modelos ajustados en self.models y las etiquetas de los clusters (grupos) en self.labels para cada imagen.\n",
    "\n",
    "Función transform:\n",
    "- Transformación de datos: El método transform() aplica el modelo entrenado a las imágenes. Para cada imagen, se agrupan los píxeles utilizando el modelo guardado en self.models.\n",
    "- Extracción de colores: Para cada grupo (etiqueta label), se calcula el color promedio de los píxeles en ese grupo.\n",
    "Se almacenan los resultados de agrupación de colores en cluster_results y cluster_image_data (que también incluye las posiciones de los píxeles agrupados)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7PGTyEnAE-aV"
   },
   "outputs": [],
   "source": [
    "# Constructor de clusters\n",
    "class ColorCluster(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, param_grid, method='kmeans', scoring_metric='silhouette', squashed=False, **kwargs):\n",
    "        self.method = method\n",
    "        self.scoring_metric = scoring_metric\n",
    "        self.param_grid = param_grid\n",
    "        self.squashed = squashed\n",
    "        self.kwargs = kwargs\n",
    "        self.models = []\n",
    "        self.labels = []\n",
    "\n",
    "    # Métricas\n",
    "    def get_scoring_metric(self):\n",
    "        metrics = {\n",
    "            'silhouette': silhouette_score,\n",
    "            'calinski_harabasz': calinski_harabasz_score,\n",
    "            'davies_bouldin': davies_bouldin_score\n",
    "        }\n",
    "        return metrics.get(self.scoring_metric, silhouette_score)\n",
    "\n",
    "    # Función auxiliar para seleccionar modelo de clustering\n",
    "    def _get_model(self):\n",
    "        models = {\n",
    "            'kmeans': KMeans(**self.kwargs),\n",
    "            'density': DBSCAN(**self.kwargs),\n",
    "            'hierarchical': AgglomerativeClustering(**self.kwargs)\n",
    "        }\n",
    "        if self.method not in models:\n",
    "            raise ValueError(f\"Algoritmo de clustering '{self.method}' no soportado. Los métodos disponibles son 'kmeans', 'density' y 'hierarchical'\")\n",
    "        return models[self.method]\n",
    "\n",
    "    # Función de entrenamiento\n",
    "    def fit(self, X, y=None):\n",
    "        self.models = []\n",
    "        self.labels = []\n",
    "\n",
    "        for image in X:\n",
    "            if image is None:\n",
    "                self.models.append(None)\n",
    "                self.labels.append(None)\n",
    "                continue\n",
    "\n",
    "            pixels = image if self.squashed else np.array(image).reshape(-1, 3)\n",
    "            model = self._get_model()\n",
    "\n",
    "            grid_search = GridSearchCV(model, self.param_grid, scoring=self.get_scoring_metric())\n",
    "            grid_search.fit(pixels)\n",
    "            best_model = grid_search.best_estimator_\n",
    "            self.models.append(best_model)\n",
    "\n",
    "            labels = best_model.predict(pixels) if hasattr(best_model, 'predict') else best_model.labels_\n",
    "            self.labels.append(labels)\n",
    "\n",
    "        return self\n",
    "\n",
    "    # Transformación\n",
    "    def transform(self, X):\n",
    "        cluster_image_data = []\n",
    "        cluster_results = []\n",
    "\n",
    "        for i, image in enumerate(X):\n",
    "            if image is None:\n",
    "                cluster_image_data.append(None)\n",
    "                cluster_results.append(None)\n",
    "                continue\n",
    "\n",
    "            pixels = image if self.squashed else np.array(image).reshape(-1, 3)\n",
    "            model = self.models[i]\n",
    "\n",
    "            if model is None:\n",
    "                cluster_image_data.append(None)\n",
    "                cluster_results.append(None)\n",
    "                continue\n",
    "\n",
    "            labels = model.predict(pixels) if hasattr(model, 'predict') else model.labels_\n",
    "            label_class = np.unique(labels)\n",
    "\n",
    "            cluster_colors = [pixels[labels == label].mean(axis=0) for label in label_class]\n",
    "            cluster_results.append(np.array(cluster_colors))\n",
    "\n",
    "            cluster_color_results = [(pixels[labels == label], np.where(labels == label)[0], cluster_colors[i]) \n",
    "                                     for i, label in enumerate(label_class)]\n",
    "            cluster_image_data.append(cluster_color_results)\n",
    "\n",
    "        return cluster_image_data, cluster_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3XQ7FGLbdIw"
   },
   "source": [
    "## Evaluación de los métodos aplicados a las imágenes mediante métricas\n",
    "\n",
    "Iniciamos aplicando processed_images_array, el cual es el arreglo que contiene las imágenes preprocesadas, cada una de las cuales ha sido redimensionada y normalizada. Su forma original es (n_imágenes, altura, ancho, 3), donde:\n",
    "\n",
    "- n_imágenes: El número de imágenes en el conjunto.\n",
    "- altura y ancho: Las dimensiones de cada imagen.\n",
    "- \n",
    "Recordemos que los tres canales de color son Rojo, Verde, Azul para cada imagen.\n",
    "\n",
    "- reshape(processed_images_array.shape[0], -1):\n",
    "\n",
    "Ahora aplicamos processed_images_array.shape[0] para mantener el número de imágenes (la primera dimensión del arreglo), es decir, no modifica la cantidad de imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r3NQCfoOM-Vy"
   },
   "outputs": [],
   "source": [
    "# Aplanar las imágenes procesadas para poder aplicar la métrica\n",
    "data = processed_images_array.reshape(processed_images_array.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función `silhouette_plot`:\n",
    "\n",
    "Esta función evalúa la calidad de los modelos de agrupación, como KMeans y AgglomerativeClustering, utilizando el Silhouette Score para medir la cohesión interna de los clusters. \n",
    "Los parámetros clave incluyen los datos a agrupar (`X`), el algoritmo a utilizar (`model`), y el rango de clusters (`k_min`, `k_max`). \n",
    "La función itera sobre diferentes valores de `k`, ajustando el modelo para cada uno, calculando el Silhouette Score que indica qué tan bien separados están los clusters. \n",
    "Si hay menos de dos clusters, asigna un puntaje de -1. \n",
    "Finalmente, grafica y guarda la variación del Silhouette Score en función del número de clusters, retornando la ruta de la imagen generada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6nGpYVFLbMTq"
   },
   "outputs": [],
   "source": [
    "# Métricas para la evaluación de los modelos\n",
    "def silhouette_plot(X, model, k_min=2, k_max=10):\n",
    "    if model not in ['kmeans', 'hierarchical']:\n",
    "        raise ValueError(f\"Modelo '{model}' no soportado para análisis de Silhouette Score\")\n",
    "    \n",
    "    scores = []\n",
    "\n",
    "    # Función auxiliar para seleccionar el modelo\n",
    "    def get_model(k):\n",
    "        if model == \"kmeans\":\n",
    "            return KMeans(n_clusters=k, max_iter=300, random_state=0)\n",
    "        elif model == \"hierarchical\":\n",
    "            return AgglomerativeClustering(n_clusters=k)\n",
    "\n",
    "    # Silhouette Score\n",
    "    for k in range(k_min, k_max + 1):\n",
    "        model_k = get_model(k)\n",
    "        model_k.fit(X)\n",
    "\n",
    "        # Solo se calcula el Silhouette Score si hay más de 1 cluster\n",
    "        score = silhouette_score(X, model_k.labels_) if len(set(model_k.labels_)) > 1 else -1\n",
    "        scores.append(score)\n",
    "\n",
    "    # Gráfico del Silhouette Score\n",
    "    plt.figure()\n",
    "    plt.plot(range(k_min, k_max + 1), scores, marker='o')\n",
    "    plt.xlabel('Número de clusters')\n",
    "    plt.ylabel('Silhouette Score')\n",
    "    plt.title(f'Evaluación de Silhouette Score usando {model}')\n",
    "    plt.grid()\n",
    "\n",
    "    save_path = f'silhouette_{model}.png'\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    return save_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, evaluamos el Silhouette Score para los modelos de agrupación KMeans y Clustering Jerárquico, variando el número de clusters entre 2 y 10. \n",
    "Para ambos algoritmos, se llama a la función silhouette_plot, que genera una gráfica con el Silhouette Score para cada número de clusters. Luego, las imágenes de las gráficas se cargan desde las rutas save_path_kmeans y save_path_hierarchical. Las gráficas se visualizan utilizando plt.imshow sin mostrar los ejes (axis('off')). Esto permite una comparación visual del rendimiento de los modelos en función de la calidad de sus agrupaciones, facilitando la elección del número óptimo de clusters para cada modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 957
    },
    "id": "vMduP-MMPDqS",
    "outputId": "cb0d7167-a621-465a-fb40-86caf27297c9"
   },
   "outputs": [],
   "source": [
    "# KMeans (Clusters entre 2 y 10)\n",
    "save_path_kmeans = silhouette_plot(data, \"kmeans\", 2, 10)\n",
    "\n",
    "img = plt.imread(save_path_kmeans)\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Clustering Jerárquico (Clusters entre 2 y 10)\n",
    "save_path_hierarchical = silhouette_plot(data, \"hierarchical\", 2, 10)\n",
    "\n",
    "img = plt.imread(save_path_hierarchical)\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NI2-TP1DplUo"
   },
   "source": [
    "## Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jeh9lgQbBNDM"
   },
   "source": [
    "Para visualizar datos en 2D, se utilizarán métodos de reducción de dimensionalidad como PCA (lineal) y t-SNE (no lineal). Se implementará una clase que aplicará estos métodos y también graficará los resultados. La clase `DimensionReductor` permite reducir la dimensionalidad de imágenes y visualizarlas en 2D. Utiliza el algoritmo PCA para reducir los datos, devolviendo la varianza explicada por cada componente, mientras que con t-SNE transforma los datos directamente sin calcular varianza. Además, incluye métodos para procesar matrices de imágenes, reduciendo su dimensionalidad de forma individual para cada imagen. Para ayudar a entender la contribución de cada componente de PCA, la clase puede generar una gráfica que muestra la varianza explicada por cada uno. También permite visualizar los datos reducidos en 2D, ya sea coloreando los puntos según los clusters a los que pertenecen o sin considerar la agrupación, utilizando colores predefinidos. De esta manera, se facilita la interpretación de datos complejos mediante la reducción de dimensionalidad y su representación gráfica.grupación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ftb0ztcppCG"
   },
   "outputs": [],
   "source": [
    "# Constructor para visualizar los datos por medio de reduccion de dimensionalidad\n",
    "\n",
    "class DimensionReductor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.reductor_model = None\n",
    "        self.image_variances = []\n",
    "\n",
    "    # Funciones de reducción de dimension PCA y TSNE\n",
    "    def pca(self, X_image, n_components=2):\n",
    "        pca_variance = PCA()\n",
    "        pca = PCA(n_components=n_components)\n",
    "        pca_variance.fit(X_image)\n",
    "        X_image_redux = pca.fit_transform(X_image)\n",
    "        variance = pca_variance.explained_variance_ratio_\n",
    "        return variance, X_image_redux\n",
    "\n",
    "    def tsne(self, X_image, n_components=2, perplexity=0.30, learning_rate=0.01, random_state=0, verbose=1):\n",
    "        tsne = TSNE(n_components=n_components, perplexity=perplexity, learning_rate=learning_rate, random_state=random_state, verbose=verbose)\n",
    "        X_image_redux = tsne.fit_transform(X_image)\n",
    "        return X_image_redux\n",
    "\n",
    "    def pca_images(self, X_image_matrix, n_components=2):\n",
    "        X_reduced = []\n",
    "        for image in X_image_matrix:\n",
    "            image_variance, X_image_redux = self.pca(image, n_components=n_components)\n",
    "            self.image_variances.append(image_variance)\n",
    "            X_reduced.append(X_image_redux)\n",
    "        return self.image_variances, X_reduced\n",
    "\n",
    "    def tsne_images(self, X_image_matrix, n_components=2, perplexity=30.0, learning_rate='auto', random_state=0, verbose=0):\n",
    "        X_reduced = []\n",
    "        for image in X_image_matrix:\n",
    "            X_image_redux = self.tsne(image, n_components=n_components, perplexity=perplexity,\n",
    "                                      learning_rate=learning_rate, random_state=random_state, verbose=verbose)\n",
    "            X_reduced.append(X_image_redux)\n",
    "        return X_reduced\n",
    "\n",
    "    # Plot de varianza para PCA\n",
    "    def pca_variance_plot(self, image_variance):\n",
    "        plt.figure(figsize=(8,4))\n",
    "        plt.plot(image_variance)\n",
    "        plt.xlabel('Varianza explicada por numero de componentes')\n",
    "        plt.ylabel('Porcentaje de varianza')\n",
    "        plt.title('Varianza explicada por componentes para la imagen')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    " # Scatter plot clusterizado y reducida dimensionalmente\n",
    "    def redux_plot(self, used_method, X_reduced_image, cluster_indices, colors):\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "        if len(cluster_indices) != len(colors):\n",
    "            raise ValueError(\"cluster_indices y colors deben tener la misma longitud\")\n",
    "\n",
    "        for i, color in enumerate(colors):\n",
    "            indices = cluster_indices[i]\n",
    "            ax.scatter(X_reduced_image[indices, 0], X_reduced_image[indices, 1],\n",
    "                      c=[color], label=f'Cluster {i}', alpha=0.6)\n",
    "\n",
    "        ax.set_title(f'Reducción de dimensionalidad  {used_method}')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    # Scatter plot no clusterizado y reducida dimensionalmente\n",
    "    def redux_plot_non_clustered(self, used_method, X_reduced_image, colors):\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "        ax.scatter(X_reduced_image[:, 0], X_reduced_image[:, 1], c=colors, alpha=0.6)\n",
    "        ax.set_title(f'Reducción de dimensionalidad{used_method}')\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KIH6cJBcBdaE"
   },
   "source": [
    "Cargaremos nuevamente las imágenes e implementaremos los modelos de clustering de color. Luego, compararemos los métodos de reducción de dimensionalidad mediante los siguientes gráficos: la paleta de colores de la imagen, la imagen original, y las proyecciones de los píxeles de color en dos dimensiones utilizando PCA y TSNE, tanto sin clusterización como con clusterización aplicada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZG2gU3UM-rM"
   },
   "source": [
    "Para poder aplicar todos los procesos de procesamiento, clusterización y reducción de la dimensionalidad en una única secuencia organizada de transformación, generamos un pipeline que aplique cada uno de los procesos mencionados hasta este punto.\n",
    "\n",
    "Por lo tanto, ahora procederemos a implementar una clase llamada `Color_Extractor`, que realiza la extracción y visualización de colores de imágenes utilizando un pipeline de procesamiento que incluye reducción de dimensionalidad y agrupación de colores. La clase hereda de varias otras clases como `Image_loader`, `Resizer`, `ColorConverter`, entre otras, para aplicar diferentes transformaciones y agrupaciones a las imágenes.\n",
    "\n",
    "En su constructor, `Color_Extractor` inicializa varios parámetros, como el modelo de agrupación (por ejemplo, KMeans), el rango de componentes para la reducción de dimensionalidad, y las imágenes a procesar. El pipeline de preprocesamiento de imágenes incluye la redimensión, conversión de color, normalización y aplastamiento de las imágenes. La clase también puede aplicar técnicas de reducción de dimensionalidad como PCA o t-SNE antes o después del proceso de agrupación.\n",
    "\n",
    "La clase utiliza un método para generar clusters, evaluando los datos y agrupándolos según el modelo de clustering seleccionado. Una vez que los clusters y la reducción de dimensionalidad han sido procesados, se grafican las paletas de colores obtenidas de las imágenes, mostrando visualmente los resultados en 2D, tanto con clusters como sin ellos. Además, ofrece la posibilidad de visualizar los datos originales y reducidos en espacios bidimensionadad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3t9OW8BXLYCk"
   },
   "outputs": [],
   "source": [
    "# Pipeline del procedimiento de extracción y visualización de color\n",
    "\n",
    "class Color_Extractor(Image_loader, Resizer, ColorConverter, Normalizer, Squasher, ColorCluster, DimensionReductor, BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, path, image_indices, param_grid, scoring_metric='silhouette', dim_redux=(None, None), n_components=2, model='kmeans', **kwargs):\n",
    "\n",
    "        self.model = model\n",
    "        self.param_grid = param_grid\n",
    "        self.scoring_metric = scoring_metric\n",
    "        self.path  = path\n",
    "        self.image_indices = image_indices\n",
    "        self.dim_redux = dim_redux\n",
    "        self.n_components = n_components\n",
    "        self.kwargs = kwargs\n",
    "        self.loader        = Image_loader(folder_path=self.path, image_indices = self.image_indices)\n",
    "        self.images_paths  = [os.path.join(self.path, img) for img in os.listdir(self.path)]\n",
    "        self.processed_images = self.image_processor(self.images_paths[self.image_indices[0]: self.image_indices[1]])\n",
    "        self.X_images_redux   = None\n",
    "        self.clustered_data   = None\n",
    "        self.platettes        = None\n",
    "\n",
    "    # Pipeline de preprocesamiento de los datos\n",
    "    def image_processor(self, X):\n",
    "        steps = [\n",
    "        ('resize', Resizer(size=(128, 128))),\n",
    "        ('color_convert', ColorConverter()),\n",
    "        ('normalize', Normalizer()),\n",
    "        ('squasher', Squasher())\n",
    "        ]\n",
    "        pipe = Pipeline(steps)\n",
    "        return pipe.fit_transform(X)\n",
    "\n",
    "\n",
    "    # Pipeline de reducción de dimensionalidad\n",
    "    def dim_redux_processing(self, X):\n",
    "        if self.dim_redux[0] == 'PCA':\n",
    "            dim_reductor  = DimensionReductor()\n",
    "            X_reducted = dim_reductor.pca_images(X, n_components=self.n_components)\n",
    "        elif self.dim_redux[0] == 'TSNE':\n",
    "            dim_reductor = DimensionReductor()\n",
    "            X_reducted  = dim_reductor.tsne_images(X, n_components=self.n_components)\n",
    "        return X_reducted\n",
    "\n",
    "\n",
    "    # Generador de clusters\n",
    "    def evaluate_clusters(self, X):\n",
    "        if self.dim_redux[1] == 'prev':\n",
    "            squashed = True\n",
    "        else:\n",
    "            squashed = False\n",
    "\n",
    "        cluster_generator = ColorCluster(param_grid=self.param_grid, method=self.model, scoring_metric=self.scoring_metric, squashed=squashed,**self.kwargs)\n",
    "        clustered_data, palettes = cluster_generator.fit_transform(X)\n",
    "        return palettes, clustered_data\n",
    "\n",
    "\n",
    "    # Pipeline de procesamiento con los modelos\n",
    "    def ml_processing(self):\n",
    "        if self.dim_redux[1] == 'prev':\n",
    "            self.X_images_redux = self.dim_redux_processing(self.processed_images)\n",
    "\n",
    "            if self.dim_redux[0] == 'PCA':\n",
    "                explained_variances = self.X_images_redux[0]\n",
    "                self.X_images_redux   = self.X_images_redux[1]\n",
    "            palettes, clustered_data = self.evaluate_clusters(self.X_images_redux)\n",
    "\n",
    "        elif self.dim_redux[1] == 'post':\n",
    "            palettes, clustered_data = self.evaluate_clusters(self.processed_images)\n",
    "            self.X_images_redux = self.dim_redux_processing(self.processed_images)\n",
    "\n",
    "        self.clustered_data = clustered_data\n",
    "        self.palettes = palettes\n",
    "\n",
    "\n",
    "    # Paleta de colores\n",
    "    def image_palette_plotter(self, palette):\n",
    "        if np.max(palette) > 1:\n",
    "            normalized_colors = palette / 255\n",
    "        else:\n",
    "            normalized_colors = palette\n",
    "        plt.figure(figsize=(8, 2))\n",
    "        for i, color in enumerate(normalized_colors):\n",
    "            plt.fill_between([i, i+1], 0, 1, color=color)\n",
    "        plt.xlim(0, len(normalized_colors))\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    # Visualización de la reducción de dimensionalidad\n",
    "    def visualizer(self):\n",
    "        self.ml_processing()\n",
    "        order_redux = self.dim_redux[1]\n",
    "        X_images = self.X_images_redux\n",
    "        X_processed_images = self.processed_images\n",
    "        palettes = self.palettes\n",
    "\n",
    "        for i, image in enumerate(X_images):\n",
    "            cluster_data = self.clustered_data[i]\n",
    "            indices_clusters = [data[1] for data in cluster_data]\n",
    "\n",
    "            if order_redux == 'post':\n",
    "                colors = [data[2] for data in cluster_data]\n",
    "                palette_ejemplo = palettes[i]\n",
    "            elif order_redux == 'prev':\n",
    "                colors_redux = [[X_processed_images[i,index] for index in clusters] for clusters in indices_clusters]\n",
    "                palette_ejemplo = [np.mean(color, axis=0) for color in colors_redux]\n",
    "                colors = palette_ejemplo\n",
    "\n",
    "            # Graficación de la imagen y su paleta\n",
    "            self.image_palette_plotter(palette_ejemplo)\n",
    "            self.loader.plotter(index_image=i)\n",
    "\n",
    "            # Graficación reducida 2D de la imagen en color no clusterizado\n",
    "            original_data = X_processed_images[i]\n",
    "            dim_reductor  = DimensionReductor()\n",
    "            dim_reductor.redux_plot_non_clustered(used_method=self.dim_redux[0], X_reduced_image=image, colors=original_data)\n",
    "\n",
    "            # Visualización de la reducción de la dimensioalidad de las imagenes\n",
    "            flattened_indices = np.concatenate(indices_clusters)\n",
    "            flattened_colors = np.concatenate(colors)\n",
    "            dim_reductor.redux_plot(used_method=self.dim_redux[0], X_reduced_image=image, cluster_indices=indices_clusters, colors=colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BijGFIRlMi9R"
   },
   "source": [
    "Para seleccionar los clusters, los mejores valores obtenidos con el Silhouette Score están entre 5 y 7, excluyendo los dos primeros clusters que dieron los resultados más óptimos. Con estos valores, se creó un **param_grid** para evaluar las imágenes en diferentes modelos.\n",
    "\n",
    "Ahora procedemos a crear una visualización comparativa de dos imágenes, una para el modelo KMeans y otra para el modelo Clustering Jerárquico, mostrando los resultados del análisis de Silhouette Score. Utiliza `plt.subplots` para generar dos subgráficos dispuestos en una fila con tamaño ajustado. \n",
    "\n",
    "Luego, cargamos las imágenes de las gráficas guardadas previamente en `save_path_kmeans` y `save_path_hierarchical`, mostrando cada una en su respectivo subgráfico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 432
    },
    "id": "pJ622ePCM7GC",
    "outputId": "76334a91-3f86-49b4-c9a2-9728e2a424d8"
   },
   "outputs": [],
   "source": [
    "# Mostrar las imágenes una al lado de la otra\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Leer las imágenes y mostrarlas en los subplots\n",
    "images = [save_path_kmeans, save_path_hierarchical]\n",
    "titles = [\"KMeans\", \"Hierarchical\"]\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    img = plt.imread(images[i])\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(titles[i])\n",
    "    ax.axis('off')\n",
    "\n",
    "# Añadir título general\n",
    "plt.suptitle('Silhouette Score para n_clusters entre 2 y 10', fontsize=16)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5pYWjkq35LY"
   },
   "source": [
    "Tras discutir cómo funciona el método de extracción de la paleta de colores, generamos resultados para una muestra aleatoria, donde el número de colores se determina automáticamente mediante Grid Search. Cada resultado incluye los siguientes gráficos: la paleta de colores, la imagen original, los píxeles de color en un contexto de dimensión reducida y los píxeles de color en dimensión reducida con clusterización aplicada.\n",
    "\n",
    "Ahorta bien, debemos implementar un modelo KMeans con reducción de dimensionalidad mediante PCA antes de aplicar el agrupamiento. El proceso comienza midiendo el tiempo de ejecución con `time.time()` con la finalidad de terminar el tiempo que tarda en ejecutar el algoritmo y así tener un parámetro de referencia para considerar que un modelo es mejor o no que otro. \n",
    "\n",
    "Luego, se especifica una carpeta de imágenes y se define un rango de imágenes para procesar. Luego, se crea un pipeline con el objeto `Color_Extractor`, que aplica PCA para reducir la dimensionalidad a 2 componentes antes de ejecutar KMeans con una búsqueda de parámetros (`n_clusters` y `init`). El modelo se ajusta utilizando el Silhouette Score para evaluar la calidad de los clusters. \n",
    "\n",
    "Finalmente, se llama al método `visualizer` para mostrar los resultados de la agrupación y se imprime el tiempo total de ejecución del proceso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1ig27NSX05k3",
    "outputId": "54084e4f-ce26-41b6-e6c4-52e696bea198"
   },
   "outputs": [],
   "source": [
    "# Modelo KMEANS con PCA\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "image_folder = \"imagenes\"\n",
    "indices_imagenes = (0, 1)  # Usar una imagen (de la primera a la segunda imagen)\n",
    "\n",
    "\n",
    "image_paths = [os.path.join(image_folder, img) for img in os.listdir(image_folder)]\n",
    "\n",
    "# Prueba del modelo kmeans con PCA\n",
    "param_grid = {\n",
    "    'n_clusters': [5,6,7],\n",
    "    'init': ['k-means++', 'random']\n",
    "}\n",
    "\n",
    "color_extractor = Color_Extractor(image_folder , indices_imagenes , param_grid=param_grid, scoring_metric='silhouette',\n",
    "                                  dim_redux=('PCA', 'prev'), n_components=2, model='kmeans')\n",
    "color_extractor.visualizer()\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Tiempo de ejecución: {execution_time} segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora procederemos a implementar un modelo KMeans con reducción de dimensionalidad mediante t-SNE antes de aplicar el agrupamiento. Se inicia estableciendo un rango de imágenes para procesar, y el modelo KMeans se ajusta con diferentes valores de `n_clusters` y métodos de inicialización. El pipeline de preprocesamiento utiliza t-SNE para reducir la dimensionalidad de las imágenes a 2 componentes antes de realizar la agrupación. \n",
    "El proceso es gestionado por la clase `Color_Extractor`, que incluye la visualización de los resultados mediante el método `visualizer()`. \n",
    "Finalmente, se mide y muestra el tiempo de ejecución total del proceso, desde el inicio hasta el final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "_8FDgInnNXzH",
    "outputId": "af3071d8-ccd9-40e2-a596-c9a32c0c2587"
   },
   "outputs": [],
   "source": [
    "# KMEANS CON TSNE\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "indices_imagenes = (0, 1)  # Usar una imagen (de la primera a la segunda imagen)\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'n_clusters': [5,6,7],\n",
    "    'init': ['k-means++', 'random']\n",
    "}\n",
    "\n",
    "color_extractor = Color_Extractor(image_folder , indices_imagenes , param_grid=param_grid , dim_redux=('TSNE', 'prev'), n_components=2, model='kmeans')\n",
    "color_extractor.visualizer()\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Tiempo de ejecución: {execution_time} segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora procederemos a implementar el modelo DBSCAN con reducción de dimensionalidad mediante PCA antes del agrupamiento. \n",
    "Se define un conjunto de parámetros, incluyendo `eps`, `min_samples`, y `metric`, para ajustar el modelo DBSCAN a través de una búsqueda en el grid. \n",
    "El pipeline de preprocesamiento aplica PCA para reducir la dimensionalidad de las imágenes a 2 componentes antes de realizar la agrupación. El proceso es gestionado por la clase `Color_Extractor`, que combina la reducción de dimensionalidad y el modelo DBSCAN. Posteriormente, se visualizan los resultados utilizando el método `visualizer()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "cJHvKBzkIFQm",
    "outputId": "fb16b9b5-300d-4e12-c7f1-e9fef144df1e"
   },
   "outputs": [],
   "source": [
    "#DBSCAN con PCA\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "indices_imagenes = (0, 1)  # Usar una imagen (de la primera a la segunda imagen)\n",
    "\n",
    "param_grid = {\n",
    "    'eps': [0.01, 0.1, 1],\n",
    "    'min_samples': [5, 10, 20],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "color_extractor = Color_Extractor(image_folder , indices_imagenes , param_grid=param_grid , dim_redux=('PCA', 'prev'), n_components=2, model='density')\n",
    "color_extractor.visualizer()\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Tiempo de ejecución: {execution_time} segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora aplicaremos el modelo DBSCAN con reducción de dimensionalidad utilizando t-SNE antes de realizar el agrupamiento. Se definen los parámetros para el modelo DBSCAN, como `eps`, `min_samples` y `metric`, los cuales se ajustan mediante una búsqueda de hiperparámetros. El pipeline de preprocesamiento utiliza t-SNE para reducir la dimensionalidad de las imágenes a 2 componentes antes de la agrupación. La clase `Color_Extractor` gestiona todo el proceso y visualiza los resultados a través del método `visualizer()`. Finalmente, el código mide el tiempo total de ejecución del proceso y lo imprime, lo que permite evaluar tanto la eficiencia como el rendimiento del modelo DBSCAN combinado con la reducción de dimensionalidad mediante t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "wiemRA6iIwP8",
    "outputId": "ec44eebd-d403-435e-d56a-2b57bf262917"
   },
   "outputs": [],
   "source": [
    "#DBSCAN con TSNE \n",
    "start_time = time.time()\n",
    "\n",
    "param_grid = {\n",
    "    'eps': [0.01, 0.1, 1],\n",
    "    'min_samples': [5, 10, 20],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "indices_imagenes = (0, 1)  # Usar una imagen (de la primera a la segunda imagen)\n",
    "\n",
    "color_extractor = Color_Extractor(image_folder , indices_imagenes , param_grid=param_grid , dim_redux=('TSNE', 'prev'), n_components=2, model='density')\n",
    "color_extractor.visualizer()\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Tiempo de ejecución: {execution_time} segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, lo que haremos será implementar un modelo de agrupamiento jerárquico utilizando reducción de dimensionalidad con PCA. \n",
    "Para esto, se define un conjunto de parámetros (`n_clusters`, `linkage` y `affinity`) que se ajustan mediante una búsqueda de hiperparámetros. El proceso aplica PCA para reducir la dimensionalidad de las imágenes a 2 componentes antes de realizar la agrupación jerárquica. La clase `Color_Extractor` gestiona tanto la reducción de dimensionalidad como la ejecución del modelo, visualizando los resultados a través de su método `visualizer()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "m_fN6yr9JdkZ",
    "outputId": "ab5d8c25-701a-45c8-95ef-5985834fb2d3"
   },
   "outputs": [],
   "source": [
    "#Modelo Jerárquico con PCA\n",
    "start_time = time.time()\n",
    "\n",
    "param_grid_agglomerative = {\n",
    "    'n_clusters': [5, 6, 7],\n",
    "    'linkage': ['ward', 'complete', 'average', 'single'],\n",
    "    'affinity': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "indices_imagenes = (0, 1)  # Usar una imagen (de la primera a la segunda imagen)\n",
    "\n",
    "\n",
    "color_extractor = Color_Extractor(image_folder , indices_imagenes , param_grid=param_grid_agglomerative , dim_redux=('PCA', 'prev'), n_components=2, model='hierarchical')\n",
    "color_extractor.visualizer()\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Tiempo de ejecución: {execution_time} segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, implementamos un modelo de agrupamiento jerárquico con reducción de dimensionalidad mediante t-SNE antes de aplicar el agrupamiento. Se define un conjunto de parámetros (`n_clusters`, `linkage` y `affinity`) para ajustar el modelo de agrupamiento jerárquico. La reducción de dimensionalidad a 2 componentes se realiza utilizando t-SNE, y el proceso completo es manejado por la clase `Color_Extractor`. El método `visualizer()` se utiliza para mostrar los resultados de la agrupación y la reducción de dimensionalidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Somzqub2J1yl",
    "outputId": "7201b118-c414-4f3f-a65c-19d232ee75a1"
   },
   "outputs": [],
   "source": [
    "#Modelo Jerárquico con TSNE\n",
    "start_time = time.time()\n",
    "\n",
    "param_grid_agglomerative = {\n",
    "    'n_clusters': [5, 6, 7],\n",
    "    'linkage': ['ward', 'complete', 'average', 'single'],\n",
    "    'affinity': ['euclidean', 'manhattan']\n",
    "}\n",
    "indices_imagenes = (0, 1)  # Usar una imagen (de la primera a la segunda imagen)\n",
    "\n",
    "\n",
    "color_extractor = Color_Extractor(image_folder , indices_imagenes , param_grid=param_grid_agglomerative, dim_redux=('TSNE', 'prev'), n_components=2, model='hierarchical')\n",
    "color_extractor.visualizer()\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Tiempo de ejecución: {execution_time} segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "De acuerdo a las ejecuciones hechas en este notebook, es posible concluir que el mejor modelo ejecutado fue el KMEANS con PCA debido a que mostró un mejor perfil de color de acuerdo a la imagen procesada, permitiendo verificar una efectiva reducción de la dimensionalidad a partir de un amplio espectro de colores.\n",
    "En cuanto a la dimensión de la imagen, para este caso escogimos (128,128) pixeles dado que se considera equilibrada en términos de resolución, información y optimización de tiempos de ejecución de los algoritmos. Si bien se pudo escoger una dimensión (256,256) se considera que este tamaño complejiza los modelos y a la hora de aplicar las respectivas funciones de extraccion de paleta. \n",
    "Por otro lado, podemos hablar de optimizar los algoritmos reduciendo las funcionalidades del presente trabajo y considerando solo aquellas que hagan un debido procesamiento de la imagen, manejando las transformaciones de manera más óptima, pero consideramos que estas son las funciones mínimas que se deben implementar para no perder información en el procesamiento de las imagenes, transformación de información y aplicación de los algoritmos no supervisados. \n",
    "En cuanto al modelo DBSCAN con PCA, este podria considerarse a la hora de definir una paleta a decisión del usuario, ya que no se limita a unos colores estándar específicos sino que muestra la variación de izquierda a derecha de los colores. Si bien el tiempo de ejecución no es el menor, computacionalmente hablando es un algoritmo efectivo en comparación con el modelo jerárquico con TSNE. \n",
    "Ahora bien, si consideramos la reducción de dimensionalidad del DBSCAN con PCA, observamos que logramos discernir clusters de manera definida, a pesar de que se solapan debido muy posiblemente a la resolución de la imagen.\n",
    "Sin embargo, el modelo más óptimo sin duda fue el KMEANS con PCA debido a su tiempo de ejecución, su rendimiento a la hora de ejecutar y la posterior visualización de los clusters esperados, lo cual indicó que se hizo una efectiva reducción de la dimensionalidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Método escogido aplicado a 4 imágenes aleatorias:\n",
    "\n",
    "### KMEANS con PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMEANS CON PCA\n",
    "param_grid = {\n",
    "    'n_clusters': [5,6,7],\n",
    "    'init': ['k-means++', 'random']\n",
    "}\n",
    "\n",
    "color_extractor = Color_Extractor(image_folder , tuple(random.sample(range(100), 4)) , param_grid=param_grid, scoring_metric='silhouette',\n",
    "                                  dim_redux=('PCA', 'prev'), n_components=2, model='kmeans')\n",
    "color_extractor.visualizer()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
